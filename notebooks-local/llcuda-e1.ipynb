{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/Local-Llama-Inference/Local-Llama-Inference.git@v0.1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T12:18:59.958556Z","iopub.execute_input":"2026-02-24T12:18:59.958815Z","iopub.status.idle":"2026-02-24T12:19:03.476014Z","shell.execute_reply.started":"2026-02-24T12:18:59.958793Z","shell.execute_reply":"2026-02-24T12:19:03.475135Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Local-Llama-Inference/Local-Llama-Inference.git@v0.1.0\n  Cloning https://github.com/Local-Llama-Inference/Local-Llama-Inference.git (to revision v0.1.0) to /tmp/pip-req-build-y1dt0lk3\n  Running command git clone --filter=blob:none --quiet https://github.com/Local-Llama-Inference/Local-Llama-Inference.git /tmp/pip-req-build-y1dt0lk3\n  Running command git checkout -q ed5888b9b98e6efff2c1f296f3267b293c89eeaa\n  Resolved https://github.com/Local-Llama-Inference/Local-Llama-Inference.git to commit ed5888b9b98e6efff2c1f296f3267b293c89eeaa\n\u001b[31mERROR: git+https://github.com/Local-Llama-Inference/Local-Llama-Inference.git@v0.1.0 does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install git+https://github.com/llcuda/llcuda.git@v2.2.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T12:40:48.573156Z","iopub.execute_input":"2026-02-24T12:40:48.573829Z","iopub.status.idle":"2026-02-24T12:40:58.721116Z","shell.execute_reply.started":"2026-02-24T12:40:48.573796Z","shell.execute_reply":"2026-02-24T12:40:58.720408Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/llcuda/llcuda.git@v2.2.0\n  Cloning https://github.com/llcuda/llcuda.git (to revision v2.2.0) to /tmp/pip-req-build-4bps3rqb\n  Running command git clone --filter=blob:none --quiet https://github.com/llcuda/llcuda.git /tmp/pip-req-build-4bps3rqb\n  Running command git checkout -q 605d1f09e83dde0151d56b8f803fa987a4799e52\n  Resolved https://github.com/llcuda/llcuda.git to commit 605d1f09e83dde0151d56b8f803fa987a4799e52\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from llcuda==2.2.0) (2.0.2)\nRequirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llcuda==2.2.0) (2.32.4)\nRequirement already satisfied: huggingface_hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from llcuda==2.2.0) (1.4.1)\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from llcuda==2.2.0) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (2025.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (1.2.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (0.28.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (6.0.3)\nRequirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (1.5.4)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (0.21.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.0->llcuda==2.2.0) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llcuda==2.2.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llcuda==2.2.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llcuda==2.2.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llcuda==2.2.0) (2026.1.4)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.20.0->llcuda==2.2.0) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.20.0->llcuda==2.2.0) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.20.0->llcuda==2.2.0) (0.16.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub>=0.20.0->llcuda==2.2.0) (8.3.1)\nBuilding wheels for collected packages: llcuda\n  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llcuda: filename=llcuda-2.2.0-py3-none-any.whl size=134471 sha256=13e12969d71450963d95499928b0144d084fa7758dcbfa163b69d14fb5e70b18\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ep00ic85/wheels/ab/b6/23/4c1e8ccb9d03e99b242626edc725471055781cdb4354b070ac\nSuccessfully built llcuda\nInstalling collected packages: llcuda\nSuccessfully installed llcuda-2.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import llcuda\nfrom huggingface_hub import hf_hub_download\n\n# Download small GGUF model (1B-5B range)\nmodel_path = hf_hub_download(\n    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n    filename=\"gemma-3-1b-it-Q4_K_M.gguf\",\n    local_dir=\"/kaggle/working/models\"\n)\n\n# Load on GPU 0 (15GB VRAM)\nengine = llcuda.InferenceEngine()\nengine.load_model(model_path, silent=True)\nresult = engine.infer(\"What is AI?\", max_tokens=100)\nprint(result.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T12:41:37.063081Z","iopub.execute_input":"2026-02-24T12:41:37.063831Z","iopub.status.idle":"2026-02-24T12:42:31.301404Z","shell.execute_reply.started":"2026-02-24T12:41:37.063796Z","shell.execute_reply":"2026-02-24T12:42:31.300689Z"}},"outputs":[{"name":"stderr","text":"WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llcuda v2.2.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llcuda v2.1!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e907a65256432f9e02ba3415958eb6"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n‚úÖ Binaries installed successfully!\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"202d492de77d4a6c9db1ae2fdeec5090"}},"metadata":{}},{"name":"stdout","text":"Loading model: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n‚úì Using local model: gemma-3-1b-it-Q4_K_M.gguf\n\nAuto-configuring optimal settings...\n‚úì Auto-configured for 15.0 GB VRAM\n  GPU Layers: 99\n  Context Size: 4096\n  Batch Size: 2048\n  Micro-batch Size: 512\n\nStarting llama-server...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready....... ‚úì Ready in 4.0s\n\n‚úì Model loaded and ready for inference\n  Server: http://127.0.0.1:8090\n  GPU Layers: 99\n  Context Size: 4096\n\n\nAI stands for Artificial Intelligence. It‚Äôs essentially about creating machines that can perform tasks that typically require human intelligence. These tasks include things like:\n\n*   **Learning:**  AI systems can learn from data without being explicitly programmed.\n*   **Problem-solving:** They can analyze situations and find solutions.\n*   **Decision-making:** They can make choices based on available information.\n*   **Understanding language:** AI can interpret and respond to human language.\n*   \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}