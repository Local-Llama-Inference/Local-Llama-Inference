{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Multi-GPU Tensor Parallelism\n",
    "\n",
    "Demonstrates how to distribute a large model across multiple GPUs using tensor parallelism.\n",
    "\n",
    "## Features\n",
    "- **GPU Detection**: Automatically detect all available GPUs\n",
    "- **Tensor Split**: Automatic layer distribution across GPUs\n",
    "- **NCCL Communication**: Multi-GPU communication via NVIDIA NCCL\n",
    "- **Optimal Configuration**: Suggestions for best settings\n",
    "- **Scaling**: Linear performance improvement with more GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import (\n",
    "    LlamaServer,\n",
    "    LlamaClient,\n",
    "    detect_gpus,\n",
    "    suggest_tensor_split,\n",
    ")\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"\u2705 Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Detect Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all GPUs\n",
    "gpus = detect_gpus()\n",
    "\n",
    "print(f\"\ud83c\udfae Detected {len(gpus)} GPU(s):\\n\")\n",
    "\n",
    "total_vram = 0\n",
    "for i, gpu in enumerate(gpus):\n",
    "    print(f\"GPU {i}: {gpu['name']}\")\n",
    "    print(f\"  Compute Capability: {gpu['compute_capability']}\")\n",
    "    print(f\"  VRAM: {gpu['memory_mb']} MB ({gpu['memory_mb']/1024:.2f} GB)\")\n",
    "    print(f\"  UUID: {gpu['uuid']}\")\n",
    "    print()\n",
    "    total_vram += gpu['memory_mb']\n",
    "\n",
    "print(f\"\ud83d\udcca Total VRAM: {total_vram} MB ({total_vram/1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Tensor Split Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal tensor split\n",
    "tensor_split = suggest_tensor_split(gpus)\n",
    "\n",
    "print(\"\ud83d\udca1 Recommended Tensor Split:\\n\")\nif isinstance(tensor_split, list):\n",
    "    for i, split in enumerate(tensor_split):\n",
    "        print(f\"  GPU {i}: {split*100:.1f}%\")\nelse:\n",
    "    print(f\"  Single GPU (only 1 GPU detected)\")\n",
    "\nprint(f\"\\n\ud83d\udcdd Tensor Split Explanation:\")\nprint(f\"  - Distributes model layers across GPUs\")\nprint(f\"  - Proportional to GPU VRAM\")\nprint(f\"  - Requires NVIDIA NCCL for communication\")\nprint(f\"  - Linear scaling: 2x GPUs \u2248 2x faster (ideal case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# For multi-GPU, larger models show better speedup\n",
    "# Using Phi-2 as example (works on single or multi-GPU)\n",
    "print(\"\ud83d\udce5 Downloading model...\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Model ready: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Server with Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Starting server with multi-GPU support...\\n\")\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    # Multi-GPU configuration\n",
    "    server = LlamaServer(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=33,\n",
    "        tensor_split=tensor_split,  # Distribute across GPUs\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f\"\ud83d\udccd Using {len(gpus)} GPUs with tensor parallelism\")\nelse:\n",
    "    # Single GPU configuration\n",
    "    server = LlamaServer(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=33,\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f\"\ud83d\udccd Using 1 GPU\")\n",
    "\nserver.start()\nprint(\"\u23f3 Waiting for server to be ready...\")\nserver.wait_ready(timeout=60)\nprint(f\"\u2705 Server ready at {server.base_url}\")\n",
    "\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify GPU Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"\ud83d\udcca GPU Utilization (nvidia-smi):\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    # Show last 15 lines (GPU info)\n",
    "    lines = result.stdout.split('\\n')\n",
    "    for line in lines[-15:]:\n",
    "        if line.strip():\n",
    "            print(line)\nexcept:\n",
    "    print(\"nvidia-smi not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Inference with Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time",
    "",
    "# Test single inference",
    "prompt = \"What are the benefits of using multiple GPUs for machine learning?\"",
    "",
    "print(f\"\ud83e\udd16 Generating response (multi-GPU)...\\n\")",
    "print(f\"Prompt: {prompt}\\n\")",
    "",
    "start_time = time.time()",
    "",
    "response = client.chat(",
    "    messages=[",
    "        {\"role\": \"user\", \"content\": prompt}",
    "    ],",
    "    max_tokens=200,",
    "    temperature=0.7,",
    ")",
    "",
    "elapsed = time.time() - start_time",
    "",
    "answer = response['choices'][0]['message']['content']",
    "print(f\"Response: {answer}\\n\")",
    "",
    "print(f\"\u23f1\ufe0f  Generation time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Benchmark Multi-GPU vs Single-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time",
    "",
    "# Simple benchmark with multiple prompts",
    "prompts = [",
    "    \"Explain Python in one sentence.\",",
    "    \"What is machine learning?\",",
    "    \"Describe cloud computing.\",",
    "]",
    "",
    "print(\"\u23f1\ufe0f  Benchmarking multi-GPU inference...\\n\")",
    "",
    "total_time = 0",
    "total_tokens = 0",
    "",
    "for i, prompt in enumerate(prompts, 1):",
    "    start_time = time.time()",
    "    ",
    "    response = client.chat(",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],",
    "        max_tokens=100,",
    "    )",
    "    ",
    "    elapsed = time.time() - start_time",
    "    tokens = response.get('usage', {}).get('completion_tokens', 0) if hasattr(response, 'usage') else 100",
    "    ",
    "    total_time += elapsed",
    "    total_tokens += tokens",
    "    ",
    "    print(f\"[{i}] Time: {elapsed:.2f}s | {prompt[:40]}...\")",
    "",
    "avg_time = total_time / len(prompts)",
    "throughput = total_tokens / total_time if total_time > 0 else 0",
    "",
    "print(f\"\\n\ud83d\udcca Benchmark Results:\")",
    "print(f\"  Total time: {total_time:.2f} seconds\")",
    "print(f\"  Average per prompt: {avg_time:.2f} seconds\")",
    "print(f\"  Throughput: {throughput:.1f} tokens/second\")",
    "print(f\"  Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Multi-Turn Conversation with Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcac Multi-turn conversation with multi-GPU:\\n\")",
    "",
    "messages = [",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}",
    "]",
    "",
    "# Turn 1",
    "user_input = \"What is distributed computing?\"",
    "print(f\"User: {user_input}\")",
    "messages.append({\"role\": \"user\", \"content\": user_input})",
    "",
    "response = client.chat(",
    "    messages=messages,",
    "    max_tokens=100,",
    ")",
    "",
    "assistant_message = response['choices'][0]['message']['content']",
    "print(f\"Assistant: {assistant_message}\\n\")",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})",
    "",
    "# Turn 2",
    "user_input = \"How does it relate to GPUs?\"",
    "print(f\"User: {user_input}\")",
    "messages.append({\"role\": \"user\", \"content\": user_input})",
    "",
    "response = client.chat(",
    "    messages=messages,",
    "    max_tokens=100,",
    ")",
    "",
    "assistant_message = response['choices'][0]['message']['content']",
    "print(f\"Assistant: {assistant_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\uded1 Stopping server...\")\nserver.stop()\nprint(\"\u2705 Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "### Tensor Parallelism\n",
    "- **Distribution**: Splits model layers across GPUs\n",
    "- **Communication**: Uses NVIDIA NCCL for GPU-to-GPU communication\n",
    "- **Scaling**: Better for very large models\n",
    "- **Throughput**: Linear improvement with more GPUs (ideal case)\n",
    "\n",
    "### When to Use Multi-GPU\n",
    "- Model doesn't fit on single GPU\n",
    "- Want faster inference\n",
    "- Have multiple identical GPUs\n",
    "- Running production workloads\n",
    "\n",
    "### Performance Considerations\n",
    "- **PCIe Bandwidth**: GPU-to-GPU communication overhead\n",
    "- **Network**: Consider if GPUs are on different machines\n",
    "- **Model Size**: Very large models show better scaling\n",
    "- **Batch Size**: Larger batches improve GPU utilization\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "Use `nvidia-smi` to monitor GPU utilization:\n",
    "```bash\n",
    "nvidia-smi dmon  # Real-time monitoring\n",
    "nvidia-smi       # One-time snapshot\n",
    "```\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `05_advanced_api.ipynb` - All 30+ API endpoints\n",
    "- `06_gpu_detection.ipynb` - Detailed GPU information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}