{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Multi-GPU Tensor Parallelism\n",
    "\n",
    "Demonstrates how to distribute a large model across multiple GPUs using tensor parallelism.\n",
    "\n",
    "## Features\n",
    "- **GPU Detection**: Automatically detect all available GPUs\n",
    "- **Tensor Split**: Automatic layer distribution across GPUs\n",
    "- **NCCL Communication**: Multi-GPU communication via NVIDIA NCCL\n",
    "- **Optimal Configuration**: Suggestions for best settings\n",
    "- **Scaling**: Linear performance improvement with more GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import (\n",
    "    LlamaServer,\n",
    "    LlamaClient,\n",
    "    detect_gpus,\n",
    "    suggest_tensor_split,\n",
    ")\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"âœ… Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Detect Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all GPUs\n",
    "gpus = detect_gpus()\n",
    "\n",
    "print(f\"ðŸŽ® Detected {len(gpus)} GPU(s):\\n\")\n",
    "\n",
    "total_vram = 0\n",
    "for i, gpu in enumerate(gpus):\n",
    "    print(f\"GPU {i}: {gpu['name']}\")\n",
    "    print(f\"  Compute Capability: {gpu['compute_capability']}\")\n",
    "    print(f\"  VRAM: {gpu['memory_mb']} MB ({gpu['memory_mb']/1024:.2f} GB)\")\n",
    "    print(f\"  UUID: {gpu['uuid']}\")\n",
    "    print()\n",
    "    total_vram += gpu['memory_mb']\n",
    "\n",
    "print(f\"ðŸ“Š Total VRAM: {total_vram} MB ({total_vram/1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Tensor Split Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal tensor split\n",
    "tensor_split = suggest_tensor_split(gpus)\n",
    "\n",
    "print(\"ðŸ’¡ Recommended Tensor Split:\\n\")\nif isinstance(tensor_split, list):\n",
    "    for i, split in enumerate(tensor_split):\n",
    "        print(f\"  GPU {i}: {split*100:.1f}%\")\nelse:\n",
    "    print(f\"  Single GPU (only 1 GPU detected)\")\n",
    "\nprint(f\"\\nðŸ“ Tensor Split Explanation:\")\nprint(f\"  - Distributes model layers across GPUs\")\nprint(f\"  - Proportional to GPU VRAM\")\nprint(f\"  - Requires NVIDIA NCCL for communication\")\nprint(f\"  - Linear scaling: 2x GPUs â‰ˆ 2x faster (ideal case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# For multi-GPU, larger models show better speedup\n",
    "# Using Phi-2 as example (works on single or multi-GPU)\n",
    "print(\"ðŸ“¥ Downloading model...\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model ready: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Server with Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting server with multi-GPU support...\\n\")\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    # Multi-GPU configuration\n",
    "    server = LlamaServer(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=33,\n",
    "        tensor_split=tensor_split,  # Distribute across GPUs\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f\"ðŸ“ Using {len(gpus)} GPUs with tensor parallelism\")\nelse:\n",
    "    # Single GPU configuration\n",
    "    server = LlamaServer(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=33,\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(f\"ðŸ“ Using 1 GPU\")\n",
    "\nserver.start()\nprint(\"â³ Waiting for server to be ready...\")\nserver.wait_ready(timeout=60)\nprint(f\"âœ… Server ready at {server.base_url}\")\n",
    "\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify GPU Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"ðŸ“Š GPU Utilization (nvidia-smi):\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    # Show last 15 lines (GPU info)\n",
    "    lines = result.stdout.split('\\n')\n",
    "    for line in lines[-15:]:\n",
    "        if line.strip():\n",
    "            print(line)\nexcept:\n",
    "    print(\"nvidia-smi not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Inference with Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test single inference\n",
    "prompt = \"What are the benefits of using multiple GPUs for machine learning?\"\n",
    "\n",
    "print(f\"ðŸ¤– Generating response (multi-GPU)...\\n\")\nprint(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"Response: {answer}\\n\")\n",
    "\n",
    "print(f\"â±ï¸  Generation time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Benchmark Multi-GPU vs Single-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simple benchmark with multiple prompts\n",
    "prompts = [\n",
    "    \"Explain Python in one sentence.\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Describe cloud computing.\",\n",
    "]\n",
    "\n",
    "print(\"â±ï¸  Benchmarking multi-GPU inference...\\n\")\n",
    "\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 100\n",
    "    \n",
    "    total_time += elapsed\n",
    "    total_tokens += tokens\n",
    "    \n",
    "    print(f\"[{i}] Time: {elapsed:.2f}s | {prompt[:40]}...\")\n",
    "\n",
    "avg_time = total_time / len(prompts)\n",
    "throughput = total_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“Š Benchmark Results:\")\nprint(f\"  Total time: {total_time:.2f} seconds\")\nprint(f\"  Average per prompt: {avg_time:.2f} seconds\")\nprint(f\"  Throughput: {throughput:.1f} tokens/second\")\nprint(f\"  Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Multi-Turn Conversation with Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ’¬ Multi-turn conversation with multi-GPU:\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}\n",
    "]\n",
    "\n",
    "# Turn 1\nuser_input = \"What is distributed computing?\"\nprint(f\"User: {user_input}\")\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(f\"Assistant: {assistant_message}\\n\")\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "# Turn 2\nuser_input = \"How does it relate to GPUs?\"\nprint(f\"User: {user_input}\")\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(f\"Assistant: {assistant_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ›‘ Stopping server...\")\nserver.stop()\nprint(\"âœ… Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "### Tensor Parallelism\n",
    "- **Distribution**: Splits model layers across GPUs\n",
    "- **Communication**: Uses NVIDIA NCCL for GPU-to-GPU communication\n",
    "- **Scaling**: Better for very large models\n",
    "- **Throughput**: Linear improvement with more GPUs (ideal case)\n",
    "\n",
    "### When to Use Multi-GPU\n",
    "- Model doesn't fit on single GPU\n",
    "- Want faster inference\n",
    "- Have multiple identical GPUs\n",
    "- Running production workloads\n",
    "\n",
    "### Performance Considerations\n",
    "- **PCIe Bandwidth**: GPU-to-GPU communication overhead\n",
    "- **Network**: Consider if GPUs are on different machines\n",
    "- **Model Size**: Very large models show better scaling\n",
    "- **Batch Size**: Larger batches improve GPU utilization\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "Use `nvidia-smi` to monitor GPU utilization:\n",
    "```bash\n",
    "nvidia-smi dmon  # Real-time monitoring\n",
    "nvidia-smi       # One-time snapshot\n",
    "```\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `05_advanced_api.ipynb` - All 30+ API endpoints\n",
    "- `06_gpu_detection.ipynb` - Detailed GPU information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
