{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Streaming Responses\n",
    "\n",
    "Demonstrates token-by-token streaming for real-time text generation.\n",
    "\n",
    "## Features\n",
    "- **Streaming Chat**: See tokens as they're generated\n",
    "- **Real-time Output**: Perfect for interactive applications\n",
    "- **Python Generators**: Use Python's async/await patterns\n",
    "- **Low Latency**: Immediate feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Package imported\n"
     ]
    }
   ],
   "source": [
    "#Step 1\n",
    "from local_llama_inference import LlamaServer, LlamaClient, detect_gpus\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"\u2705 Package imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, config: local_llama_inference.config.ServerConfig, binary_path: Optional[str] = None)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from local_llama_inference import LlamaServer\n",
    "print(inspect.signature(LlamaServer.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waqasm86/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Gemma 3 Model ready at: /home/waqasm86/models/gemma-3-1b-it-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "#Step 2\n",
    "\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Define your local models directory\n",
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download Gemma-3-1b-it-Q4_K_M model if not already present\n",
    "# Using the Unsloth repository as requested\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n",
    "    filename=\"gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    "    local_dir_use_symlinks=False  # Ensures the actual file is in your models folder\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Gemma 3 Model ready at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/waqasm86/models\n"
     ]
    }
   ],
   "source": [
    "print(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting server...\n",
      "\u2705 Server ready\n"
     ]
    }
   ],
   "source": [
    "#Step 3\n",
    "\n",
    "from local_llama_inference import LlamaServer, LlamaClient, ServerConfig\n",
    "\n",
    "# model_path is already defined from Step 2\n",
    "print(\"\ud83d\ude80 Starting server...\")\n",
    "\n",
    "# Create a configuration object with your desired settings\n",
    "config = ServerConfig(\n",
    "    model_path=model_path,      # full path to your .gguf file\n",
    "    n_gpu_layers=1,             # offload 1 layer to GPU (adjust if needed)\n",
    "    n_threads=1,                 # CPU threads for prompt processing\n",
    "    # Optional: specify host/port if you want non-default values\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8090,\n",
    ")\n",
    "\n",
    "# Pass the config object to LlamaServer\n",
    "server = LlamaServer(config)\n",
    "server.start()\n",
    "server.wait_ready(timeout=60)\n",
    "print(\"\u2705 Server ready\")\n",
    "\n",
    "# Client connects to http://127.0.0.1:8090 by default\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - FIXED: stream_chat() returns Iterator[str], not chunk objects\n",
    "\n",
    "client = LlamaClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"\ud83e\udd16 Streaming response:\\n\")\n",
    "print(\"Q: Write a haiku about machine learning\\nA: \", end=\"\", flush=True)\n",
    "\n",
    "# stream_chat() yields raw token strings directly\n",
    "for token in client.stream_chat(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about machine learning\"}],\n",
    "    max_tokens=100,\n",
    "):\n",
    "    # token is a raw string, just print it\n",
    "    if token:\n",
    "        print(token, end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use client.chat() for non-streaming (returns dict)\n",
    "response = client.chat(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about machine learning\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "# chat() returns a dict (OpenAI-compatible JSON)\n",
    "print(\"Assistant:\", response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available methods in LlamaClient\n",
    "print(\"Available methods (no chat_completion in v0.1.0):\")\n",
    "methods = [m for m in dir(LlamaClient) if not m.startswith('_')]\n",
    "for method in sorted(methods):\n",
    "    print(f\"  - {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaClient methods:\n",
      "['apply_template', 'chat', 'close', 'complete', 'detokenize', 'embed', 'erase_slot', 'get_lora_adapters', 'get_metrics', 'get_models', 'get_props', 'get_slots', 'health', 'infill', 'load_model', 'rerank', 'restore_slot', 'save_slot', 'set_lora_adapters', 'set_props', 'stream_chat', 'stream_complete', 'tokenize', 'unload_model']\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually inside LlamaClient\n",
    "from local_llama_inference import LlamaClient\n",
    "print(\"LlamaClient methods:\")\n",
    "print([method for method in dir(LlamaClient) if not method.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 0.1.0\n",
      "Installed at: /home/waqasm86/.local/lib/python3.11/site-packages/local_llama_inference/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import local_llama_inference\n",
    "print(f\"SDK version: {local_llama_inference.__version__}\")\n",
    "print(f\"Installed at: {local_llama_inference.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaClient:\n",
      "    \"\"\"Synchronous HTTP client for llama-server REST API.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        base_url: str = \"http://127.0.0.1:8080\",\n",
      "        api_key: Optional[str] = None,\n",
      "        timeout: float = 600.0,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize client.\n",
      "\n",
      "        Args:\n",
      "            base_url: Base URL of llama-server\n",
      "            api_key: Optional API key for authentication\n",
      "            timeout: Request timeout in seconds\n",
      "        \"\"\"\n",
      "        self.base_url = base_url.rstrip(\"/\")\n",
      "        self._headers = {}\n",
      "        if api_key:\n",
      "            self._headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
      "        self._client = httpx.Client(headers=self._headers, timeout=timeout)\n",
      "\n",
      "    def _request(\n",
      "        self,\n",
      "        method: str,\n",
      "        endpoint: str,\n",
      "        **kwargs,\n",
      "    ) -> httpx.Response:\n",
      "        \"\"\"Make HTTP request.\"\"\"\n",
      "        url = f\"{self.base_url}{endpoint}\"\n",
      "        try:\n",
      "            response = self._client.request(method, url, **kwargs)\n",
      "            response.raise_for_status()\n",
      "            return response\n",
      "        except httpx.HTTPStatusError as e:\n",
      "            try:\n",
      "                data = e.response.json()\n",
      "                msg = data.get(\"error\", {}).get(\"message\", str(e))\n",
      "            except:\n",
      "                msg = str(e)\n",
      "            raise APIError(e.response.status_code, msg) from e\n",
      "        except httpx.RequestError as e:\n",
      "            raise ClientError(str(e)) from e\n",
      "\n",
      "    # Health & Info Endpoints\n",
      "    def health(self) -> Dict[str, Any]:\n",
      "        \"\"\"GET /health - Server health status.\"\"\"\n",
      "        return self._request(\"GET\", \"/health\").json()\n",
      "\n",
      "    def get_props(self) -> Dict[str, Any]:\n",
      "        \"\"\"GET /props - Server properties.\"\"\"\n",
      "        return self._request(\"GET\", \"/props\").json()\n",
      "\n",
      "    def set_props(self, props: Dict[str, Any]) -> Dict[str, Any]:\n",
      "        \"\"\"POST /props - Update server properties.\"\"\"\n",
      "        return self._request(\"POST\", \"/props\", json=props).json()\n",
      "\n",
      "    def get_models(self) -> Dict[str, Any]:\n",
      "        \"\"\"GET /models - List loaded models.\"\"\"\n",
      "        return self._request(\"GET\", \"/models\").json()\n",
      "\n",
      "    def get_slots(self, fail_on_no_slot: bool = False) -> List[Dict]:\n",
      "        \"\"\"GET /slots - Get inference slots status.\"\"\"\n",
      "        params = {\"fail_on_no_slot\": fail_on_no_slot}\n",
      "        return self._request(\"GET\", \"/slots\", params=params).json()\n",
      "\n",
      "    def get_metrics(self) -> str:\n",
      "        \"\"\"GET /metrics - Prometheus metrics.\"\"\"\n",
      "        return self._request(\"GET\", \"/metrics\").text\n",
      "\n",
      "    def get_lora_adapters(self) -> List[Dict]:\n",
      "        \"\"\"GET /lora-adapters - List loaded LoRA adapters.\"\"\"\n",
      "        return self._request(\"GET\", \"/lora-adapters\").json()\n",
      "\n",
      "    def set_lora_adapters(self, adapters: List[Dict]) -> Dict:\n",
      "        \"\"\"POST /lora-adapters - Set LoRA adapter scales.\"\"\"\n",
      "        return self._request(\"POST\", \"/lora-adapters\", json=adapters).json()\n",
      "\n",
      "    # Text Generation Endpoints\n",
      "    def complete(self, prompt: str, stream: bool = False, **kwargs) -> Dict | Iterator:\n",
      "        \"\"\"\n",
      "        POST /completion - Text completion.\n",
      "\n",
      "        Args:\n",
      "            prompt: Text prompt\n",
      "            stream: Stream response tokens\n",
      "            **kwargs: Additional parameters (temperature, top_p, etc.)\n",
      "\n",
      "        Returns:\n",
      "            Dict if stream=False, Iterator[str] if stream=True\n",
      "        \"\"\"\n",
      "        payload = {\"prompt\": prompt, \"stream\": stream, **kwargs}\n",
      "        if stream:\n",
      "            return self._stream_response(self._request(\"POST\", \"/completion\", json=payload))\n",
      "        return self._request(\"POST\", \"/completion\", json=payload).json()\n",
      "\n",
      "    def chat(self, messages: List[Dict], stream: bool = False, **kwargs) -> Dict | Iterator:\n",
      "        \"\"\"\n",
      "        POST /v1/chat/completions - Chat completion (OpenAI-compatible).\n",
      "\n",
      "        Args:\n",
      "            messages: List of message dicts with 'role' and 'content'\n",
      "            stream: Stream response\n",
      "            **kwargs: Additional parameters\n",
      "\n",
      "        Returns:\n",
      "            Dict if stream=False, Iterator[str] if stream=True\n",
      "        \"\"\"\n",
      "        payload = {\"messages\": messages, \"stream\": stream, **kwargs}\n",
      "        if stream:\n",
      "            return self._stream_response(\n",
      "                self._request(\"POST\", \"/v1/chat/completions\", json=payload)\n",
      "            )\n",
      "        return self._request(\"POST\", \"/v1/chat/completions\", json=payload).json()\n",
      "\n",
      "    def stream_chat(self, messages: List[Dict], **kwargs) -> Iterator[str]:\n",
      "        \"\"\"Stream chat completion tokens.\"\"\"\n",
      "        yield from self.chat(messages, stream=True, **kwargs)\n",
      "\n",
      "    def stream_complete(self, prompt: str, **kwargs) -> Iterator[str]:\n",
      "        \"\"\"Stream completion tokens.\"\"\"\n",
      "        yield from self.complete(prompt, stream=True, **kwargs)\n",
      "\n",
      "    def infill(self, prefix: str, suffix: str, **kwargs) -> Dict:\n",
      "        \"\"\"POST /infill - Fill-in-the-middle code completion.\"\"\"\n",
      "        payload = {\"input_prefix\": prefix, \"input_suffix\": suffix, **kwargs}\n",
      "        return self._request(\"POST\", \"/infill\", json=payload).json()\n",
      "\n",
      "    # Embeddings & Reranking\n",
      "    def embed(self, input: str | List[str], **kwargs) -> Dict:\n",
      "        \"\"\"POST /v1/embeddings - Generate embeddings.\"\"\"\n",
      "        payload = {\"input\": input, **kwargs}\n",
      "        return self._request(\"POST\", \"/v1/embeddings\", json=payload).json()\n",
      "\n",
      "    def rerank(self, query: str, documents: List[str], top_n: Optional[int] = None) -> Dict:\n",
      "        \"\"\"POST /v1/rerank - Rerank documents.\"\"\"\n",
      "        payload = {\"query\": query, \"documents\": documents}\n",
      "        if top_n is not None:\n",
      "            payload[\"top_n\"] = top_n\n",
      "        return self._request(\"POST\", \"/v1/rerank\", json=payload).json()\n",
      "\n",
      "    # Token Utilities\n",
      "    def tokenize(\n",
      "        self,\n",
      "        content: str,\n",
      "        add_special: bool = True,\n",
      "        parse_special: bool = False,\n",
      "        with_pieces: bool = False,\n",
      "    ) -> Dict:\n",
      "        \"\"\"POST /tokenize - Tokenize text.\"\"\"\n",
      "        payload = {\n",
      "            \"content\": content,\n",
      "            \"add_special\": add_special,\n",
      "            \"parse_special\": parse_special,\n",
      "            \"with_pieces\": with_pieces,\n",
      "        }\n",
      "        return self._request(\"POST\", \"/tokenize\", json=payload).json()\n",
      "\n",
      "    def detokenize(self, tokens: List[int]) -> Dict:\n",
      "        \"\"\"POST /detokenize - Detokenize token IDs.\"\"\"\n",
      "        return self._request(\"POST\", \"/detokenize\", json={\"tokens\": tokens}).json()\n",
      "\n",
      "    def apply_template(self, messages: List[Dict]) -> Dict:\n",
      "        \"\"\"POST /apply-template - Apply chat template.\"\"\"\n",
      "        return self._request(\"POST\", \"/apply-template\", json={\"messages\": messages}).json()\n",
      "\n",
      "    # Model Management (Router Mode)\n",
      "    def load_model(self, model_name: str) -> Dict:\n",
      "        \"\"\"POST /models/load - Load a model.\"\"\"\n",
      "        return self._request(\"POST\", \"/models/load\", json={\"model\": model_name}).json()\n",
      "\n",
      "    def unload_model(self, model_name: str) -> Dict:\n",
      "        \"\"\"POST /models/unload - Unload a model.\"\"\"\n",
      "        return self._request(\"POST\", \"/models/unload\", json={\"model\": model_name}).json()\n",
      "\n",
      "    # Slot Management\n",
      "    def save_slot(self, slot_id: int, filename: str) -> Dict:\n",
      "        \"\"\"POST /slots/{slot_id}?action=save - Save KV cache.\"\"\"\n",
      "        return self._request(\n",
      "            \"POST\",\n",
      "            f\"/slots/{slot_id}\",\n",
      "            params={\"action\": \"save\"},\n",
      "            json={\"filename\": filename},\n",
      "        ).json()\n",
      "\n",
      "    def restore_slot(self, slot_id: int, filename: str) -> Dict:\n",
      "        \"\"\"POST /slots/{slot_id}?action=restore - Restore KV cache.\"\"\"\n",
      "        return self._request(\n",
      "            \"POST\",\n",
      "            f\"/slots/{slot_id}\",\n",
      "            params={\"action\": \"restore\"},\n",
      "            json={\"filename\": filename},\n",
      "        ).json()\n",
      "\n",
      "    def erase_slot(self, slot_id: int) -> Dict:\n",
      "        \"\"\"POST /slots/{slot_id}?action=erase - Erase KV cache.\"\"\"\n",
      "        return self._request(\"POST\", f\"/slots/{slot_id}\", params={\"action\": \"erase\"}).json()\n",
      "\n",
      "    def _stream_response(self, response: httpx.Response) -> Iterator[str]:\n",
      "        \"\"\"Parse SSE stream response.\"\"\"\n",
      "        with response:\n",
      "            for line in response.iter_lines():\n",
      "                if line.startswith(\"data: \") and line != \"data: [DONE]\":\n",
      "                    try:\n",
      "                        chunk = json.loads(line[6:])\n",
      "                        delta = chunk.get(\"choices\", [{}])[0].get(\"delta\", {})\n",
      "                        if content := delta.get(\"content\", \"\"):\n",
      "                            yield content\n",
      "                    except json.JSONDecodeError:\n",
      "                        continue\n",
      "\n",
      "    def close(self):\n",
      "        \"\"\"Close HTTP connection.\"\"\"\n",
      "        self._client.close()\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, *args):\n",
      "        self.close()\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return f\"LlamaClient({self.base_url})\"\n",
      "\n",
      "Help on class LlamaClient in module local_llama_inference.client:\n",
      "\n",
      "class LlamaClient(builtins.object)\n",
      " |  LlamaClient(base_url: str = 'http://127.0.0.1:8080', api_key: Optional[str] = None, timeout: float = 600.0)\n",
      " |  \n",
      " |  Synchronous HTTP client for llama-server REST API.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  __init__(self, base_url: str = 'http://127.0.0.1:8080', api_key: Optional[str] = None, timeout: float = 600.0)\n",
      " |      Initialize client.\n",
      " |      \n",
      " |      Args:\n",
      " |          base_url: Base URL of llama-server\n",
      " |          api_key: Optional API key for authentication\n",
      " |          timeout: Request timeout in seconds\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply_template(self, messages: List[Dict]) -> Dict\n",
      " |      POST /apply-template - Apply chat template.\n",
      " |  \n",
      " |  chat(self, messages: List[Dict], stream: bool = False, **kwargs) -> Union[Dict, Iterator]\n",
      " |      POST /v1/chat/completions - Chat completion (OpenAI-compatible).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of message dicts with 'role' and 'content'\n",
      " |          stream: Stream response\n",
      " |          **kwargs: Additional parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict if stream=False, Iterator[str] if stream=True\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close HTTP connection.\n",
      " |  \n",
      " |  complete(self, prompt: str, stream: bool = False, **kwargs) -> Union[Dict, Iterator]\n",
      " |      POST /completion - Text completion.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: Text prompt\n",
      " |          stream: Stream response tokens\n",
      " |          **kwargs: Additional parameters (temperature, top_p, etc.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict if stream=False, Iterator[str] if stream=True\n",
      " |  \n",
      " |  detokenize(self, tokens: List[int]) -> Dict\n",
      " |      POST /detokenize - Detokenize token IDs.\n",
      " |  \n",
      " |  embed(self, input: Union[str, List[str]], **kwargs) -> Dict\n",
      " |      POST /v1/embeddings - Generate embeddings.\n",
      " |  \n",
      " |  erase_slot(self, slot_id: int) -> Dict\n",
      " |      POST /slots/{slot_id}?action=erase - Erase KV cache.\n",
      " |  \n",
      " |  get_lora_adapters(self) -> List[Dict]\n",
      " |      GET /lora-adapters - List loaded LoRA adapters.\n",
      " |  \n",
      " |  get_metrics(self) -> str\n",
      " |      GET /metrics - Prometheus metrics.\n",
      " |  \n",
      " |  get_models(self) -> Dict[str, Any]\n",
      " |      GET /models - List loaded models.\n",
      " |  \n",
      " |  get_props(self) -> Dict[str, Any]\n",
      " |      GET /props - Server properties.\n",
      " |  \n",
      " |  get_slots(self, fail_on_no_slot: bool = False) -> List[Dict]\n",
      " |      GET /slots - Get inference slots status.\n",
      " |  \n",
      " |  health(self) -> Dict[str, Any]\n",
      " |      GET /health - Server health status.\n",
      " |  \n",
      " |  infill(self, prefix: str, suffix: str, **kwargs) -> Dict\n",
      " |      POST /infill - Fill-in-the-middle code completion.\n",
      " |  \n",
      " |  load_model(self, model_name: str) -> Dict\n",
      " |      POST /models/load - Load a model.\n",
      " |  \n",
      " |  rerank(self, query: str, documents: List[str], top_n: Optional[int] = None) -> Dict\n",
      " |      POST /v1/rerank - Rerank documents.\n",
      " |  \n",
      " |  restore_slot(self, slot_id: int, filename: str) -> Dict\n",
      " |      POST /slots/{slot_id}?action=restore - Restore KV cache.\n",
      " |  \n",
      " |  save_slot(self, slot_id: int, filename: str) -> Dict\n",
      " |      POST /slots/{slot_id}?action=save - Save KV cache.\n",
      " |  \n",
      " |  set_lora_adapters(self, adapters: List[Dict]) -> Dict\n",
      " |      POST /lora-adapters - Set LoRA adapter scales.\n",
      " |  \n",
      " |  set_props(self, props: Dict[str, Any]) -> Dict[str, Any]\n",
      " |      POST /props - Update server properties.\n",
      " |  \n",
      " |  stream_chat(self, messages: List[Dict], **kwargs) -> Iterator[str]\n",
      " |      Stream chat completion tokens.\n",
      " |  \n",
      " |  stream_complete(self, prompt: str, **kwargs) -> Iterator[str]\n",
      " |      Stream completion tokens.\n",
      " |  \n",
      " |  tokenize(self, content: str, add_special: bool = True, parse_special: bool = False, with_pieces: bool = False) -> Dict\n",
      " |      POST /tokenize - Tokenize text.\n",
      " |  \n",
      " |  unload_model(self, model_name: str) -> Dict\n",
      " |      POST /models/unload - Unload a model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(LlamaClient))  # This will print the class definition\n",
    "# Or for a cleaner view:\n",
    "help(LlamaClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaClient' object has no attribute 'chat_completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test with non-streaming chat\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m(\n\u001b[32m      3\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWrite a haiku about machine learning\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m      4\u001b[39m     max_tokens=\u001b[32m100\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssistant:\u001b[39m\u001b[33m\"\u001b[39m, response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mAttributeError\u001b[39m: 'LlamaClient' object has no attribute 'chat_completion'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Streaming Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83e\udd16 Streaming completion:\\n\")\n",
    "print(\"Prompt: The future of AI is...\\nResponse: \", end=\"\", flush=True)\n",
    "\n",
    "prompt = \"The future of AI is\"\n",
    "# stream_complete() returns Iterator[str] of raw tokens\n",
    "for token in client.stream_complete(\n",
    "    prompt=prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.8,\n",
    "):\n",
    "    # token is a raw string\n",
    "    if token:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Measuring Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\u23f1\ufe0f  Measuring token generation latency:\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "token_count = 0\n",
    "first_token_time = None\n",
    "\n",
    "print(\"Prompt: Explain quantum computing\\nResponse: \", end=\"\", flush=True)\n",
    "\n",
    "# stream_chat() yields raw token strings\n",
    "for token in client.stream_chat(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "):\n",
    "    # token is a raw string\n",
    "    if token:\n",
    "        token_count += 1\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time() - start_time\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\n\ud83d\udcca Latency Metrics:\")\n",
    "print(f\"  First token latency: {first_token_time*1000:.1f} ms\")\n",
    "print(f\"  Tokens generated: {token_count}\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "if total_time > 0:\n",
    "    print(f\"  Tokens/second: {token_count/total_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Multiple Streaming Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd04 Streaming multiple requests sequentially:\\n\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"What is CUDA?\",\n",
    "    \"What is GPU computing?\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n[{i}] Q: {prompt}\")\n",
    "    print(\"    A: \", end=\"\", flush=True)\n",
    "    \n",
    "    # stream_chat() yields raw token strings\n",
    "    for token in client.stream_chat(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=80,\n",
    "    ):\n",
    "        if token:\n",
    "            print(token, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # Newline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Interactive Streaming (Multi-turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcac Interactive streaming conversation:\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant who speaks concisely.\"}\n",
    "]\n",
    "\n",
    "# Turn 1\n",
    "user_input = \"What is recursion in programming?\"\n",
    "print(f\"\ud83d\udc64 User: {user_input}\")\n",
    "print(\"\ud83e\udd16 Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "assistant_response = \"\"\n",
    "\n",
    "# stream_chat() yields raw token strings\n",
    "for token in client.stream_chat(\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    "):\n",
    "    if token:\n",
    "        assistant_response += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "# Turn 2\n",
    "user_input = \"Can you give an example?\"\n",
    "print(f\"\ud83d\udc64 User: {user_input}\")\n",
    "print(\"\ud83e\udd16 Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "assistant_response = \"\"\n",
    "\n",
    "# stream_chat() yields raw token strings\n",
    "for token in client.stream_chat(\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    "):\n",
    "    if token:\n",
    "        assistant_response += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\uded1 Stopping server...\")\n",
    "server.stop()\n",
    "print(\"\u2705 Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "- **Streaming**: Use `stream_chat()` or `stream_complete()` for token-by-token generation\n",
    "- **Python Generators**: Iterate with `for` loop over chunks\n",
    "- **Low Latency**: First token appears quickly, visible feedback\n",
    "- **Multi-turn**: Build conversation history as you stream\n",
    "- **Parameters**: All standard parameters work with streaming (temperature, max_tokens, etc.)\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `03_embeddings.ipynb` - Generate text embeddings\n",
    "- `04_multi_gpu.ipynb` - Multi-GPU tensor parallelism\n",
    "- `05_advanced_api.ipynb` - All 30+ API endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}