{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference v0.1.0 - Quick Start\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Auto-Download of CUDA Binaries** (834 MB from Hugging Face)\n",
    "2. **GPU Detection** (NVIDIA sm_50+)\n",
    "3. **Basic Chat with LLM**\n",
    "4. **Server Management**\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "- **First Run**: Will download 834 MB CUDA binaries from Hugging Face (~10-15 minutes)\n",
    "- **Cached**: Subsequent runs use cached binaries (instant)\n",
    "- **GPU Required**: NVIDIA GPU with compute capability 5.0+ (Kepler, Maxwell, Pascal, etc.)\n",
    "- **Models**: Download GGUF models from https://huggingface.co/models?search=gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Package (if not already installed)\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/Local-Llama-Inference/Local-Llama-Inference.git@v0.1.0\n",
    "```\n",
    "\n",
    "Or from PyPI when available:\n",
    "```bash\n",
    "pip install local-llama-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Package (Triggers Auto-Download on First Import)\n",
    "\n",
    "‚ö†Ô∏è **First time only**: This will download binaries from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main classes\n",
    "from local_llama_inference import (\n",
    "    LlamaServer,\n",
    "    LlamaClient,\n",
    "    detect_gpus,\n",
    "    suggest_tensor_split,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Package imported successfully!\")\n",
    "print(\"‚úÖ CUDA binaries are now downloaded and cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Detect Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available GPUs\n",
    "gpus = detect_gpus()\n",
    "\n",
    "print(f\"\\nüéÆ Detected {len(gpus)} GPU(s):\\n\")\n",
    "for i, gpu in enumerate(gpus):\n",
    "    print(f\"GPU {i}: {gpu['name']}\")\n",
    "    print(f\"  Compute Capability: {gpu['compute_capability']}\")\n",
    "    print(f\"  VRAM: {gpu['memory_mb']} MB\")\n",
    "    print(f\"  UUID: {gpu['uuid']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download a GGUF Model\n",
    "\n",
    "For this example, we'll use a small model. Popular options:\n",
    "- **Mistral 7B Q4** (4.3 GB): TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
    "- **Phi-2 Q4** (1.4 GB): TheBloke/phi-2-GGUF  \n",
    "- **Llama 2 7B Q4** (3.8 GB): TheBloke/Llama-2-7B-Chat-GGUF\n",
    "- **Orca Mini 3B Q4** (1.9 GB): TheBloke/orca_mini-3B-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üì• Downloading Phi-2 Q4 model (1.4 GB)...\")\nprint(\"   This may take a few minutes depending on internet speed...\\n\")\n",
    "\n",
    "# Download Phi-2 Q4 model (smaller, faster)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model downloaded to: {model_path}\")\n",
    "print(f\"   File size: {Path(model_path).stat().st_size / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start LLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect GPUs for optimal configuration\n",
    "gpus = detect_gpus()\n",
    "\n",
    "print(f\"üöÄ Starting LLM server with {len(gpus)} GPU(s)...\\n\")\n",
    "\n",
    "# Create server with optimal settings\n",
    "server = LlamaServer(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=33,  # Offload layers to GPU\n",
    "    n_threads=4,      # CPU threads\n",
    "    n_ctx=2048,       # Context size\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Start the server\n",
    "server.start()\n",
    "print(\"‚è≥ Waiting for server to be ready...\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "server.wait_ready(timeout=60)\n",
    "print(f\"‚úÖ Server ready at {server.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Client and Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = LlamaClient()\n",
    "\n",
    "# Send a chat message\n",
    "print(\"ü§ñ Sending message to LLM...\\n\")\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning? Explain in 2-3 sentences.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful programming assistant.\"},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "user_message = \"Explain Python decorators in simple terms.\"\n",
    "print(f\"üë§ User: {user_message}\\n\")\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(f\"ü§ñ Assistant: {assistant_message}\\n\")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "# Second turn\n",
    "user_message = \"Can you show me a simple example?\"\n",
    "print(f\"üë§ User: {user_message}\\n\")\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(f\"ü§ñ Assistant: {assistant_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check Server Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check server health\n",
    "health = client.health()\n",
    "print(\"üìä Server Health:\")\n",
    "print(f\"  Status: {health.get('status', 'unknown')}\")\n",
    "\n",
    "# Get server properties\n",
    "props = client.get_props()\n",
    "print(\"\\n‚öôÔ∏è  Server Properties:\")\n",
    "print(f\"  Model: {props.get('default_generation_settings', {}).get('model', 'unknown')}\")\n",
    "print(f\"  Context Size: {props.get('default_generation_settings', {}).get('n_ctx', 'unknown')}\")\n",
    "print(f\"  GPU Layers: {props.get('default_generation_settings', {}).get('n_gpu_layers', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Stop Server (Cleanup)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Always stop the server when done to free GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "print(\"üõë Stopping server...\")\n",
    "server.stop()\n",
    "print(\"‚úÖ Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **You've successfully:**\n",
    "1. Auto-downloaded CUDA binaries from Hugging Face (one-time setup)\n",
    "2. Detected your GPU and its capabilities\n",
    "3. Downloaded a GGUF model\n",
    "4. Started a local LLM server\n",
    "5. Created a multi-turn conversation\n",
    "6. Checked server health\n",
    "7. Properly cleaned up resources\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check out `02_streaming_responses.ipynb` for token-by-token generation\n",
    "- See `03_embeddings.ipynb` for text embeddings\n",
    "- Try `04_multi_gpu.ipynb` for multi-GPU tensor parallelism\n",
    "- Explore `05_advanced_api.ipynb` for all available endpoints\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **GitHub**: https://github.com/Local-Llama-Inference/Local-Llama-Inference\n",
    "- **Models**: https://huggingface.co/models?search=gguf\n",
    "- **Binaries**: https://huggingface.co/datasets/waqasm86/Local-Llama-Inference/\n",
    "- **Documentation**: Check README.md in the repository"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
