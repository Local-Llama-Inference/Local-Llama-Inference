{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Advanced API Endpoints\n",
    "\n",
    "Comprehensive guide to all 30+ API endpoints available in local-llama-inference.\n",
    "\n",
    "## API Categories\n",
    "1. **Chat & Completions** - Text generation\n",
    "2. **Embeddings** - Vector representations\n",
    "3. **Tokenization** - Token operations\n",
    "4. **Server Management** - Health & status\n",
    "5. **Advanced** - Infill, reranking, LoRA\n",
    "6. **Configuration** - Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import LlamaServer, LlamaClient, detect_gpus\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "print(\"âœ… Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Download Model and Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "# Start server\n",
    "print(\"ðŸš€ Starting server...\")\nserver = LlamaServer(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=33,\n",
    "    n_threads=4,\n",
    ")\n",
    "server.start()\n",
    "server.wait_ready(timeout=60)\n",
    "print(f\"âœ… Server ready\\n\")\n",
    "\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chat & Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"1. CHAT & COMPLETIONS API\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 1.1 Basic Chat Completion\nprint(\"\\n1.1 Chat Completion (Non-streaming)\")\nresponse = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=50,\n",
    ")\nprint(f\"Response: {response.choices[0].message.content}\")\n",
    "\n",
    "# 1.2 Streaming Chat\nprint(\"\\n1.2 Chat Completion (Streaming)\")\nprint(\"Response: \", end=\"\", flush=True)\nfor chunk in client.stream_chat(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}],\n",
    "    max_tokens=50,\n",
    "):\n",
    "    token = chunk.choices[0].delta.content\n",
    "    if token:\n",
    "        print(token, end=\"\", flush=True)\nprint(\"\\n\")\n",
    "\n",
    "# 1.3 Text Completion\nprint(\"1.3 Text Completion\")\nresponse = client.complete(\n",
    "    prompt=\"The future of AI is\",\n",
    "    max_tokens=50,\n",
    ")\nprint(f\"Completion: {response.choices[0].text}\")\n",
    "\n",
    "# 1.4 Streaming Completion\nprint(\"\\n1.4 Streaming Completion\")\nprint(\"Response: \", end=\"\", flush=True)\nfor chunk in client.stream_complete(\n",
    "    prompt=\"Python is a programming language that\",\n",
    "    max_tokens=40,\n",
    "):\n",
    "    token = chunk.choices[0].text\n",
    "    if token:\n",
    "        print(token, end=\"\", flush=True)\nprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"2. SAMPLING PARAMETERS\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 2.1 Temperature (creativity)\nprint(\"\\n2.1 Temperature Control (creativity)\")\nfor temp in [0.1, 0.5, 1.0]:\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Complete: The sky is\"}],\n",
    "        temperature=temp,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    text = response.choices[0].message.content[:50]\n",
    "    print(f\"  Temp={temp}: {text}...\")\n",
    "\n",
    "# 2.2 Top-P (nucleus sampling)\nprint(\"\\n2.2 Top-P Sampling\")\nresponse = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say something\"}],\n",
    "    top_p=0.9,\n",
    "    max_tokens=30,\n",
    ")\nprint(f\"Response: {response.choices[0].message.content[:60]}...\")\n",
    "\n",
    "# 2.3 Top-K Sampling\nprint(\"\\n2.3 Top-K Sampling\")\nresponse = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fact\"}],\n",
    "    top_k=40,\n",
    "    max_tokens=30,\n",
    ")\nprint(f\"Response: {response.choices[0].message.content[:60]}...\")\n",
    "\n",
    "# 2.4 Repetition Penalty\nprint(\"\\n2.4 Repetition Penalty\")\nresponse = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "    repeat_penalty=1.5,\n",
    "    max_tokens=30,\n",
    ")\nprint(f\"Response: {response.choices[0].message.content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"3. EMBEDDINGS & TOKENIZATION\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 3.1 Generate Embeddings\nprint(\"\\n3.1 Generate Embeddings\")\nresponse = client.embed(input=\"Hello world\")\nembedding = response.data[0]['embedding']\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n",
    "\n",
    "# 3.2 Multiple Embeddings\nprint(\"\\n3.2 Multiple Embeddings\")\nresponse = client.embed(input=[\"AI is great\", \"Machine learning is fun\"])\nprint(f\"Generated {len(response.data)} embeddings\")\nfor i, item in enumerate(response.data):\n",
    "    print(f\"  Embedding {i}: dimension {len(item['embedding'])}\")\n",
    "\n",
    "# 3.3 Tokenization\nprint(\"\\n3.3 Tokenize Text\")\nresponse = client.tokenize(content=\"Hello world, how are you?\")\nprint(f\"Text: 'Hello world, how are you?'\")\nprint(f\"Tokens: {response.tokens}\")\nprint(f\"Token count: {len(response.tokens)}\")\n",
    "\n",
    "# 3.4 Detokenization\nprint(\"\\n3.4 Detokenize Tokens\")\nresponse = client.detokenize(tokens=[1, 2, 3, 4, 5])\nprint(f\"Tokens [1,2,3,4,5] -> '{response.content}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Server Status & Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"4. SERVER STATUS & HEALTH\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 4.1 Health Check\nprint(\"\\n4.1 Server Health\")\nhealth = client.health()\nprint(f\"Status: {health.get('status', 'unknown')}\")\n",
    "\n",
    "# 4.2 Server Properties\nprint(\"\\n4.2 Server Properties\")\nprops = client.get_props()\nprint(json.dumps(props, indent=2)[:500] + \"...\")\n",
    "\n",
    "# 4.3 Server Metrics\nprint(\"\\n4.3 Server Metrics\")\ntry:\n",
    "    metrics = client.get_metrics()\n",
    "    print(f\"Metrics: {metrics}\")\nexcept:\n",
    "    print(\"Metrics not available on this server version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"5. ADVANCED FEATURES\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 5.1 Apply Chat Template\nprint(\"\\n5.1 Apply Chat Template\")\ntry:\n",
    "    response = client.apply_template(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Formatted prompt: {response.prompt[:100]}...\")\nexcept:\n",
    "    print(\"Chat template not available\")\n",
    "\n",
    "# 5.2 Code Infill (if supported)\nprint(\"\\n5.2 Code Infill\")\ntry:\n",
    "    response = client.infill(\n",
    "        prompt=\"def hello():\\n    print(\",\n",
    "        suffix=\")\\n    return True\",\n",
    "    )\n",
    "    print(f\"Infilled code: {response}\")\nexcept:\n",
    "    print(\"Code infill not available\")\n",
    "\n",
    "# 5.3 Reranking\nprint(\"\\n5.3 Reranking\")\ntry:\n",
    "    response = client.rerank(\n",
    "        query=\"What is machine learning?\",\n",
    "        documents=[\n",
    "            \"ML is a subset of AI\",\n",
    "            \"Deep learning uses neural networks\",\n",
    "            \"The weather is sunny today\",\n",
    "        ]\n",
    "    )\n",
    "    print(\"Reranked results:\")\n",
    "    for result in response.results:\n",
    "        print(f\"  {result}\")\nexcept:\n",
    "    print(\"Reranking not available\")\n",
    "\n",
    "# 5.4 LoRA Adapters\nprint(\"\\n5.4 LoRA Adapters\")\ntry:\n",
    "    response = client.set_lora_adapters(\n",
    "        lora_adapter=[],  # Empty list to clear\n",
    "    )\n",
    "    print(f\"LoRA status: {response}\")\nexcept:\n",
    "    print(\"LoRA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"6. BATCH OPERATIONS\")\nprint(\"=\" * 60)\n",
    "\n",
    "# Process multiple requests\nrequests = [\n",
    "    \"What is Python?\",\n",
    "    \"What is CUDA?\",\n",
    "    \"What is machine learning?\",\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(requests)} requests...\\n\")\n",
    "\n",
    "results = []\n",
    "for i, prompt in enumerate(requests, 1):\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    results.append({\"prompt\": prompt, \"answer\": answer})\n",
    "    print(f\"[{i}] {prompt}\")\n",
    "    print(f\"    {answer[:80]}...\\n\")\n",
    "\n",
    "print(f\"Processed {len(results)} requests successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"7. ERROR HANDLING\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 7.1 Invalid parameters\nprint(\"\\n7.1 Handling Invalid Parameters\")\ntry:\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=-100,  # Invalid: negative\n",
    "    )\nexcept Exception as e:\n",
    "    print(f\"Error caught: {type(e).__name__}\")\n",
    "    print(f\"Message: {str(e)[:100]}\")\n",
    "\n",
    "# 7.2 Valid request succeeds\nprint(\"\\n7.2 Valid Request\")\ntry:\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    print(f\"âœ… Success: Got response with {response.usage.completion_tokens} tokens\")\nexcept Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Reference Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"API REFERENCE SUMMARY\")\nprint(\"=\" * 60)\n",
    "\n",
    "api_endpoints = {\n",
    "    \"Chat & Completions\": [\n",
    "        \"chat_completion(messages, ...)\",\n",
    "        \"stream_chat(messages, ...)\",\n",
    "        \"complete(prompt, ...)\",\n",
    "        \"stream_complete(prompt, ...)\",\n",
    "    ],\n",
    "    \"Embeddings & Tokens\": [\n",
    "        \"embed(input)\",\n",
    "        \"tokenize(content)\",\n",
    "        \"detokenize(tokens)\",\n",
    "        \"apply_template(messages)\",\n",
    "    ],\n",
    "    \"Advanced\": [\n",
    "        \"infill(prompt, suffix)\",\n",
    "        \"rerank(query, documents)\",\n",
    "        \"set_lora_adapters(lora_adapter)\",\n",
    "    ],\n",
    "    \"Server\": [\n",
    "        \"health()\",\n",
    "        \"get_props()\",\n",
    "        \"get_metrics()\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for category, methods in api_endpoints.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for method in methods:\n",
    "        print(f\"  â€¢ {method}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\nprint(\"See LlamaClient source for complete parameter documentation\")\nprint(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ›‘ Stopping server...\")\nserver.stop()\nprint(\"âœ… Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parameters Across Endpoints\n",
    "\n",
    "### Generation Parameters\n",
    "- `max_tokens`: Maximum tokens to generate\n",
    "- `temperature`: 0.0 (deterministic) to 1.0+ (creative)\n",
    "- `top_p`: Nucleus sampling (0.0-1.0)\n",
    "- `top_k`: Top-K sampling (0-100)\n",
    "- `repeat_penalty`: Penalize repetition (1.0-2.0)\n",
    "- `frequency_penalty`: Frequency-based penalty\n",
    "- `presence_penalty`: Presence-based penalty\n",
    "\n",
    "### Advanced Parameters\n",
    "- `n_predict`: Alias for max_tokens\n",
    "- `seed`: Random seed for reproducibility\n",
    "- `stop`: Stop sequences (list of strings)\n",
    "- `logit_bias`: Bias logits for specific tokens\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Use streaming** for real-time feedback\n",
    "2. **Batch requests** to improve throughput\n",
    "3. **Cache embeddings** for semantic search\n",
    "4. **Monitor health** for production systems\n",
    "5. **Handle errors** gracefully\n",
    "6. **Use appropriate models** for embeddings vs generation\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `06_gpu_detection.ipynb` - Detailed GPU analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
