{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Advanced API Endpoints\n",
    "\n",
    "Comprehensive guide to all 30+ API endpoints available in local-llama-inference.\n",
    "\n",
    "## API Categories\n",
    "1. **Chat & Completions** - Text generation\n",
    "2. **Embeddings** - Vector representations\n",
    "3. **Tokenization** - Token operations\n",
    "4. **Server Management** - Health & status\n",
    "5. **Advanced** - Infill, reranking, LoRA\n",
    "6. **Configuration** - Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import LlamaServer, LlamaClient, detect_gpus\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "print(\"\u2705 Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Download Model and Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "# Start server\n",
    "print(\"\ud83d\ude80 Starting server...\")\nserver = LlamaServer(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=33,\n",
    "    n_threads=4,\n",
    ")\n",
    "server.start()\n",
    "server.wait_ready(timeout=60)\n",
    "print(f\"\u2705 Server ready\\n\")\n",
    "\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chat & Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"1. CHAT & COMPLETIONS API\")",
    "print(\"=\" * 60)",
    "",
    "# 1.1 Basic Chat Completion",
    "print(\"\\n1.1 Chat Completion (Non-streaming)\")",
    "response = client.chat(",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],",
    "    temperature=0.7,",
    "    max_tokens=50,",
    ")",
    "print(f\"Response: {response['choices'][0]['message']['content']}\")",
    "",
    "# 1.2 Streaming Chat",
    "print(\"\\n1.2 Chat Completion (Streaming)\")",
    "print(\"Response: \", end=\"\", flush=True)",
    "for chunk in client.stream_chat(",
    "    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}],",
    "    max_tokens=50,",
    "):",
    "    token = chunk",
    "    if token:",
    "        print(token, end=\"\", flush=True)",
    "print(\"\\n\")",
    "",
    "# 1.3 Text Completion",
    "print(\"1.3 Text Completion\")",
    "response = client.complete(",
    "    prompt=\"The future of AI is\",",
    "    max_tokens=50,",
    ")",
    "print(f\"Completion: {response['choices'][0].get('text', '')}\")",
    "",
    "# 1.4 Streaming Completion",
    "print(\"\\n1.4 Streaming Completion\")",
    "print(\"Response: \", end=\"\", flush=True)",
    "for chunk in client.stream_complete(",
    "    prompt=\"Python is a programming language that\",",
    "    max_tokens=40,",
    "):",
    "    token = chunk['choices'][0].get('text', '')",
    "    if token:",
    "        print(token, end=\"\", flush=True)",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"2. SAMPLING PARAMETERS\")",
    "print(\"=\" * 60)",
    "",
    "# 2.1 Temperature (creativity)",
    "print(\"\\n2.1 Temperature Control (creativity)\")",
    "for temp in [0.1, 0.5, 1.0]:",
    "    response = client.chat(",
    "        messages=[{\"role\": \"user\", \"content\": \"Complete: The sky is\"}],",
    "        temperature=temp,",
    "        max_tokens=20,",
    "    )",
    "    text = response['choices'][0]['message']['content'][:50]",
    "    print(f\"  Temp={temp}: {text}...\")",
    "",
    "# 2.2 Top-P (nucleus sampling)",
    "print(\"\\n2.2 Top-P Sampling\")",
    "response = client.chat(",
    "    messages=[{\"role\": \"user\", \"content\": \"Say something\"}],",
    "    top_p=0.9,",
    "    max_tokens=30,",
    ")",
    "print(f\"Response: {response['choices'][0]['message']['content'][:60]}...\")",
    "",
    "# 2.3 Top-K Sampling",
    "print(\"\\n2.3 Top-K Sampling\")",
    "response = client.chat(",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fact\"}],",
    "    top_k=40,",
    "    max_tokens=30,",
    ")",
    "print(f\"Response: {response['choices'][0]['message']['content'][:60]}...\")",
    "",
    "# 2.4 Repetition Penalty",
    "print(\"\\n2.4 Repetition Penalty\")",
    "response = client.chat(",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],",
    "    repeat_penalty=1.5,",
    "    max_tokens=30,",
    ")",
    "print(f\"Response: {response['choices'][0]['message']['content'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"3. EMBEDDINGS & TOKENIZATION\")",
    "print(\"=\" * 60)",
    "",
    "# 3.1 Generate Embeddings",
    "print(\"\\n3.1 Generate Embeddings\")",
    "response = client.embed(input=\"Hello world\")",
    "embedding = response['data'][0]['embedding']",
    "print(f\"Embedding dimension: {len(embedding)}\")",
    "print(f\"First 5 values: {embedding[:5]}\")",
    "",
    "# 3.2 Multiple Embeddings",
    "print(\"\\n3.2 Multiple Embeddings\")",
    "response = client.embed(input=[\"AI is great\", \"Machine learning is fun\"])",
    "print(f\"Generated {len(response.data)} embeddings\")",
    "for i, item in enumerate(response.data):",
    "    print(f\"  Embedding {i}: dimension {len(item['embedding'])}\")",
    "",
    "# 3.3 Tokenization",
    "print(\"\\n3.3 Tokenize Text\")",
    "response = client.tokenize(content=\"Hello world, how are you?\")",
    "print(f\"Text: 'Hello world, how are you?'\")",
    "print(f\"Tokens: {response.get('tokens', [])}\")",
    "print(f\"Token count: {len(response.get('tokens', []))}\")",
    "",
    "# 3.4 Detokenization",
    "print(\"\\n3.4 Detokenize Tokens\")",
    "response = client.detokenize(tokens=[1, 2, 3, 4, 5])",
    "print(f\"Tokens [1,2,3,4,5] -> '{response.content}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Server Status & Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"4. SERVER STATUS & HEALTH\")\nprint(\"=\" * 60)\n",
    "\n",
    "# 4.1 Health Check\nprint(\"\\n4.1 Server Health\")\nhealth = client.health()\nprint(f\"Status: {health.get('status', 'unknown')}\")\n",
    "\n",
    "# 4.2 Server Properties\nprint(\"\\n4.2 Server Properties\")\nprops = client.get_props()\nprint(json.dumps(props, indent=2)[:500] + \"...\")\n",
    "\n",
    "# 4.3 Server Metrics\nprint(\"\\n4.3 Server Metrics\")\ntry:\n",
    "    metrics = client.get_metrics()\n",
    "    print(f\"Metrics: {metrics}\")\nexcept:\n",
    "    print(\"Metrics not available on this server version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"5. ADVANCED FEATURES\")",
    "print(\"=\" * 60)",
    "",
    "# 5.1 Apply Chat Template",
    "print(\"\\n5.1 Apply Chat Template\")",
    "try:",
    "    response = client.apply_template(",
    "        messages=[",
    "            {\"role\": \"user\", \"content\": \"Hello\"},",
    "            {\"role\": \"assistant\", \"content\": \"Hi there\"},",
    "        ]",
    "    )",
    "    print(f\"Formatted prompt: {response.prompt[:100]}...\")",
    "except:",
    "    print(\"Chat template not available\")",
    "",
    "# 5.2 Code Infill (if supported)",
    "print(\"\\n5.2 Code Infill\")",
    "try:",
    "    response = client.infill(",
    "        prompt=\"def hello():\\n    print(\",",
    "        suffix=\")\\n    return True\",",
    "    )",
    "    print(f\"Infilled code: {response}\")",
    "except:",
    "    print(\"Code infill not available\")",
    "",
    "# 5.3 Reranking",
    "print(\"\\n5.3 Reranking\")",
    "try:",
    "    response = client.rerank(",
    "        query=\"What is machine learning?\",",
    "        documents=[",
    "            \"ML is a subset of AI\",",
    "            \"Deep learning uses neural networks\",",
    "            \"The weather is sunny today\",",
    "        ]",
    "    )",
    "    print(\"Reranked results:\")",
    "    for result in response['results']:",
    "        print(f\"  {result}\")",
    "except:",
    "    print(\"Reranking not available\")",
    "",
    "# 5.4 LoRA Adapters",
    "print(\"\\n5.4 LoRA Adapters\")",
    "try:",
    "    response = client.set_lora_adapters(",
    "        lora_adapter=[],  # Empty list to clear",
    "    )",
    "    print(f\"LoRA status: {response}\")",
    "except:",
    "    print(\"LoRA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"6. BATCH OPERATIONS\")",
    "print(\"=\" * 60)",
    "",
    "# Process multiple requests",
    "requests = [",
    "    \"What is Python?\",",
    "    \"What is CUDA?\",",
    "    \"What is machine learning?\",",
    "]",
    "",
    "print(f\"\\nProcessing {len(requests)} requests...\\n\")",
    "",
    "results = []",
    "for i, prompt in enumerate(requests, 1):",
    "    response = client.chat(",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],",
    "        max_tokens=50,",
    "    )",
    "    answer = response['choices'][0]['message']['content']",
    "    results.append({\"prompt\": prompt, \"answer\": answer})",
    "    print(f\"[{i}] {prompt}\")",
    "    print(f\"    {answer[:80]}...\\n\")",
    "",
    "print(f\"Processed {len(results)} requests successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"7. ERROR HANDLING\")",
    "print(\"=\" * 60)",
    "",
    "# 7.1 Invalid parameters",
    "print(\"\\n7.1 Handling Invalid Parameters\")",
    "try:",
    "    response = client.chat(",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],",
    "        max_tokens=-100,  # Invalid: negative",
    "    )",
    "except Exception as e:",
    "    print(f\"Error caught: {type(e).__name__}\")",
    "    print(f\"Message: {str(e)[:100]}\")",
    "",
    "# 7.2 Valid request succeeds",
    "print(\"\\n7.2 Valid Request\")",
    "try:",
    "    response = client.chat(",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],",
    "        max_tokens=50,",
    "    )",
    "    print(f\"\u2705 Success: Got response with {response.get('usage', {}).get('completion_tokens', 0)} tokens\")",
    "except Exception as e:",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Reference Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"API REFERENCE SUMMARY\")\nprint(\"=\" * 60)\n",
    "\n",
    "api_endpoints = {\n",
    "    \"Chat & Completions\": [\n",
    "        \"chat_completion(messages, ...)\",\n",
    "        \"stream_chat(messages, ...)\",\n",
    "        \"complete(prompt, ...)\",\n",
    "        \"stream_complete(prompt, ...)\",\n",
    "    ],\n",
    "    \"Embeddings & Tokens\": [\n",
    "        \"embed(input)\",\n",
    "        \"tokenize(content)\",\n",
    "        \"detokenize(tokens)\",\n",
    "        \"apply_template(messages)\",\n",
    "    ],\n",
    "    \"Advanced\": [\n",
    "        \"infill(prompt, suffix)\",\n",
    "        \"rerank(query, documents)\",\n",
    "        \"set_lora_adapters(lora_adapter)\",\n",
    "    ],\n",
    "    \"Server\": [\n",
    "        \"health()\",\n",
    "        \"get_props()\",\n",
    "        \"get_metrics()\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for category, methods in api_endpoints.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for method in methods:\n",
    "        print(f\"  \u2022 {method}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\nprint(\"See LlamaClient source for complete parameter documentation\")\nprint(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\uded1 Stopping server...\")\nserver.stop()\nprint(\"\u2705 Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parameters Across Endpoints\n",
    "\n",
    "### Generation Parameters\n",
    "- `max_tokens`: Maximum tokens to generate\n",
    "- `temperature`: 0.0 (deterministic) to 1.0+ (creative)\n",
    "- `top_p`: Nucleus sampling (0.0-1.0)\n",
    "- `top_k`: Top-K sampling (0-100)\n",
    "- `repeat_penalty`: Penalize repetition (1.0-2.0)\n",
    "- `frequency_penalty`: Frequency-based penalty\n",
    "- `presence_penalty`: Presence-based penalty\n",
    "\n",
    "### Advanced Parameters\n",
    "- `n_predict`: Alias for max_tokens\n",
    "- `seed`: Random seed for reproducibility\n",
    "- `stop`: Stop sequences (list of strings)\n",
    "- `logit_bias`: Bias logits for specific tokens\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Use streaming** for real-time feedback\n",
    "2. **Batch requests** to improve throughput\n",
    "3. **Cache embeddings** for semantic search\n",
    "4. **Monitor health** for production systems\n",
    "5. **Handle errors** gracefully\n",
    "6. **Use appropriate models** for embeddings vs generation\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `06_gpu_detection.ipynb` - Detailed GPU analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}