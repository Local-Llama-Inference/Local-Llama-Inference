{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - Embeddings Generation\n",
    "\n",
    "Demonstrates how to generate text embeddings using local-llama-inference.\n",
    "\n",
    "## Applications\n",
    "- **Semantic Search**: Find similar documents\n",
    "- **Clustering**: Group similar texts\n",
    "- **Similarity**: Measure text similarity\n",
    "- **Reranking**: Improve search results\n",
    "- **RAG (Retrieval-Augmented Generation)**: Knowledge retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import LlamaServer, LlamaClient\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path.home() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# For embeddings, any GGUF model works\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/phi-2-GGUF\",\n",
    "    filename=\"phi-2.Q4_K_M.gguf\",\n",
    "    local_dir=str(models_dir),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model ready: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting server...\")\n",
    "server = LlamaServer(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=33,\n",
    "    n_threads=4,\n",
    ")\n",
    "server.start()\n",
    "server.wait_ready(timeout=60)\n",
    "print(f\"‚úÖ Server ready\")\n",
    "\n",
    "client = LlamaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Generate Single Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding for a single text\n",
    "text = \"Machine learning is a subset of artificial intelligence\"\n",
    "\n",
    "print(f\"üìù Text: {text}\\n\")\nprint(\"üßÆ Generating embedding...\")\n",
    "\n",
    "response = client.embed(input=text)\n",
    "\n",
    "embedding = response.data[0]['embedding']\n",
    "print(f\"‚úÖ Embedding generated\")\nprint(f\"   Dimension: {len(embedding)}\")\nprint(f\"   First 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Generate Multiple Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple texts for semantic search\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "    \"Natural language processing handles text data\",\n",
    "    \"Computer vision processes images\",\n",
    "    \"Reinforcement learning learns from rewards\",\n",
    "]\n",
    "\n",
    "print(\"üßÆ Generating embeddings for 5 texts...\\n\")\n",
    "\n",
    "response = client.embed(input=texts)\n",
    "\n",
    "embeddings = [item['embedding'] for item in response.data]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\nfor i, (text, emb) in enumerate(zip(texts, embeddings)):\n",
    "    print(f\"  [{i}] {text[:50]}... (dim={len(emb)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Semantic Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Query text\n",
    "query = \"What is deep neural networks?\"\n",
    "\n",
    "# Generate query embedding\n",
    "query_response = client.embed(input=query)\n",
    "query_embedding = query_response.data[0]['embedding']\n",
    "\n",
    "print(f\"üîç Query: {query}\\n\")\nprint(\"üìä Semantic Similarity Results:\\n\")\n",
    "\n",
    "# Calculate similarity to each text\n",
    "similarities = []\n",
    "for text, embedding in zip(texts, embeddings):\n",
    "    # Cosine similarity: 1 - cosine_distance\n",
    "    similarity = 1 - cosine(query_embedding, embedding)\n",
    "    similarities.append((text, similarity))\n",
    "\n",
    "# Sort by similarity (descending)\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display results\n",
    "for rank, (text, sim) in enumerate(similarities, 1):\n",
    "    print(f\"{rank}. {sim:.3f} - {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "print(\"üéØ Clustering documents based on embeddings...\\n\")\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "print(\"üìç Cluster Assignments:\\n\")\nfor i, (text, cluster) in enumerate(zip(texts, clusters)):\n",
    "    print(f\"Cluster {cluster}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Text Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "print(\"üìä Similarity Matrix (Cosine Similarity):\\n\")\n",
    "\n",
    "# Calculate pairwise similarities\n",
    "similarities = []\n",
    "for i, emb1 in enumerate(embeddings):\n",
    "    row = []\n",
    "    for emb2 in embeddings:\n",
    "        sim = 1 - cosine(emb1, emb2)\n",
    "        row.append(sim)\n",
    "    similarities.append(row)\n",
    "\n",
    "similarity_matrix = np.array(similarities)\n",
    "\n",
    "# Display as formatted table\n",
    "print(\"     \", end=\"\")\nfor i in range(len(texts)):\n",
    "    print(f\"  T{i} \", end=\"\")\nprint()\n",
    "\n",
    "for i, row in enumerate(similarity_matrix):\n",
    "    print(f\"T{i}  \", end=\"\")\n",
    "    for val in row:\n",
    "        print(f\"{val:0.2f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: 1.00 = identical, 0.00 = completely different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Reranking Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank documents based on relevance to query\n",
    "query = \"neural networks and deep learning\"\n",
    "\n",
    "print(f\"üîÑ Reranking documents for query: {query}\\n\")\n",
    "\n",
    "# Use reranking API if available\n",
    "try:\n",
    "    response = client.rerank(\n",
    "        query=query,\n",
    "        documents=texts,\n",
    "    )\n",
    "    \n",
    "    print(\"üèÜ Reranked Results:\\n\")\n",
    "    for rank, result in enumerate(response.results, 1):\n",
    "        idx = result['index']\n",
    "        score = result['relevance_score']\n",
    "        print(f\"{rank}. Score: {score:.3f} - {texts[idx]}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: Reranking not available in this server version\")\n",
    "    print(f\"Using embedding-based similarity instead...\\n\")\n",
    "    \n",
    "    # Fallback to embedding-based similarity\n",
    "    query_response = client.embed(input=query)\n",
    "    query_embedding = query_response.data[0]['embedding']\n",
    "    \n",
    "    results = []\n",
    "    for idx, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "        sim = 1 - cosine(query_embedding, embedding)\n",
    "        results.append((idx, text, sim))\n",
    "    \n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"üèÜ Similarity-Based Ranking:\\n\")\n",
    "    for rank, (idx, text, score) in enumerate(results, 1):\n",
    "        print(f\"{rank}. Score: {score:.3f} - {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Tokenization (Token Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens in texts\nprint(\"üìä Token Count Analysis:\\n\")\n",
    "\n",
    "for text in texts:\n",
    "    response = client.tokenize(content=text)\n",
    "    token_count = len(response.tokens)\n",
    "    print(f\"{token_count:3d} tokens: {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüõë Stopping server...\")\nserver.stop()\nprint(\"‚úÖ Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "- **Embeddings**: Use `embed()` to generate dense vector representations\n",
    "- **Similarity**: Cosine similarity compares embeddings (0 = different, 1 = identical)\n",
    "- **Multi-text**: Embed multiple texts at once for efficiency\n",
    "- **Applications**: Search, clustering, reranking, RAG systems\n",
    "- **Dimension**: Embedding dimension depends on the model\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "1. **Semantic Search**: Find similar documents by embedding similarity\n",
    "2. **Duplicate Detection**: Identify duplicate or near-duplicate texts\n",
    "3. **Clustering**: Group similar documents together\n",
    "4. **Classification**: Use embeddings as features for ML models\n",
    "5. **RAG Systems**: Retrieve relevant documents for context\n",
    "\n",
    "## Next Notebooks\n",
    "\n",
    "- `04_multi_gpu.ipynb` - Multi-GPU tensor parallelism\n",
    "- `05_advanced_api.ipynb` - All 30+ API endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
