{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-Llama-Inference - GPU Detection and Analysis\n",
    "\n",
    "Comprehensive guide to detecting GPU capabilities and optimizing configurations.\n",
    "\n",
    "## GPU Information\n",
    "- **Compute Capability**: Hardware generation (determines supported features)\n",
    "- **VRAM**: Memory available for models and computation\n",
    "- **UUID**: Unique identifier for tracking GPUs\n",
    "- **Architecture**: NVIDIA architecture (sm_50, sm_61, sm_70, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llama_inference import detect_gpus, suggest_tensor_split, check_cuda_version\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "print(\"‚úÖ Package imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"SYSTEM INFORMATION\")\nprint(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  Platform: {platform.system()} {platform.release()}\")\nprint(f\"   Processor: {platform.processor()}\")\nprint(f\"   Python Version: {platform.python_version()}\")\n",
    "\n",
    "# Check CUDA\ntry:\n",
    "    cuda_version = check_cuda_version()\n",
    "    print(f\"\\nüéÆ CUDA Setup:\")\n",
    "    if cuda_version:\n",
    "        print(f\"   ‚úÖ CUDA Available: {cuda_version}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  CUDA not found\")\nexcept Exception as e:\n",
    "    print(f\"   Error checking CUDA: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Detect GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"GPU DETECTION\")\nprint(\"=\" * 70)\n",
    "\n",
    "gpus = detect_gpus()\n",
    "\n",
    "if not gpus:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPUs detected!\")\n",
    "    print(\"   Make sure you have:\")\n",
    "    print(\"   - NVIDIA GPU installed\")\n",
    "    print(\"   - NVIDIA drivers installed\")\n",
    "    print(\"   - CUDA toolkit installed\")\nelse:\n",
    "    print(f\"\\n‚úÖ Detected {len(gpus)} GPU(s):\\n\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i}: {gpu['name']}\")\n",
    "        print(f\"  Compute Capability: {gpu['compute_capability']}\")\n",
    "        print(f\"  VRAM: {gpu['memory_mb']} MB ({gpu['memory_mb']/1024:.2f} GB)\")\n",
    "        print(f\"  UUID: {gpu['uuid']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GPU Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"GPU ARCHITECTURE ANALYSIS\")\nprint(\"=\" * 70)\n",
    "\n",
    "# NVIDIA GPU Architecture reference\narchitectures = {\n",
    "    (3, 0): (\"Kepler\", \"K80, K40\"),\n",
    "    (5, 0): (\"Maxwell\", \"GTX 750, GTX 950\"),\n",
    "    (6, 1): (\"Pascal\", \"GTX 1060, GTX 1080\"),\n",
    "    (7, 0): (\"Volta\", \"Tesla V100\"),\n",
    "    (7, 5): (\"Turing\", \"RTX 2060, RTX 2080\"),\n",
    "    (8, 0): (\"Ampere\", \"RTX 3060, RTX 3090\"),\n",
    "    (8, 6): (\"Ada\", \"RTX 4080, RTX 6000\"),\n",
    "    (8, 9): (\"Hopper\", \"H100, H200\"),\n",
    "}\n",
    "\nif gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        cc = gpu['compute_capability']\n",
    "        major, minor = cc.split('.')\n",
    "        major, minor = int(major), int(minor)\n",
    "        \n",
    "        print(f\"\\nGPU {i}: {gpu['name']}\")\n",
    "        print(f\"  Compute Capability: {cc}\")\n",
    "        \n",
    "        # Find architecture\n",
    "        arch_name = None\n",
    "        arch_examples = None\n",
    "        for (maj, min), (name, examples) in architectures.items():\n",
    "            if major == maj:\n",
    "                arch_name = name\n",
    "                arch_examples = examples\n",
    "                break\n",
    "        \n",
    "        if arch_name:\n",
    "            print(f\"  Architecture: {arch_name}\")\n",
    "            print(f\"  Examples: {arch_examples}\")\n",
    "        else:\n",
    "            print(f\"  Architecture: Unknown\")\n",
    "        \n",
    "        # Feature support\n",
    "        print(f\"\\n  Feature Support:\")\n",
    "        print(f\"    ‚úÖ CUDA Compute: Yes\")\n",
    "        print(f\"    ‚úÖ Float32: Yes\")\n",
    "        print(f\"    {'‚úÖ' if major >= 5 else '‚ùå'} Float64 (if sm_50+): Yes\")\n",
    "        print(f\"    {'‚úÖ' if major >= 7 else '‚ùå'} Tensor Cores (if sm_70+): Yes\")\n",
    "        print(f\"    {'‚úÖ' if major >= 8 else '‚ùå'} Structured Sparsity (if sm_80+): Yes\")\nelse:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: VRAM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"VRAM ANALYSIS\")\nprint(\"=\" * 70)\n",
    "\nif gpus:\n",
    "    total_vram = sum(gpu['memory_mb'] for gpu in gpus)\n",
    "    \n",
    "    print(f\"\\nTotal VRAM: {total_vram} MB ({total_vram/1024:.2f} GB)\")\n",
    "    \n",
    "    print(f\"\\nPer-GPU VRAM:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        vram_gb = gpu['memory_mb'] / 1024\n",
    "        pct = (gpu['memory_mb'] / total_vram * 100) if total_vram > 0 else 0\n",
    "        print(f\"  GPU {i}: {vram_gb:.2f} GB ({pct:.1f}%)\")\n",
    "    \n",
    "    # Model capacity estimates\n",
    "    print(f\"\\nüìä Model Capacity (Rough Estimates):\")\n",
    "    print(f\"\\n  For {total_vram/1024:.2f} GB total VRAM:\")\n",
    "    \n",
    "    model_sizes = [\n",
    "        (\"Phi-2 Q4 (1.4 GB)\", 1.4),\n",
    "        (\"Mistral 7B Q4 (4.3 GB)\", 4.3),\n",
    "        (\"Llama 2 7B Q4 (3.8 GB)\", 3.8),\n",
    "        (\"Llama 2 13B Q4 (7.3 GB)\", 7.3),\n",
    "    ]\n",
    "    \n",
    "    for model_name, size_gb in model_sizes:\n",
    "        fits = \"‚úÖ Yes\" if size_gb <= total_vram/1024 * 0.8 else \"‚ùå No (tight fit)\"\n",
    "        print(f\"    {model_name}: {fits}\")\nelse:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tensor Split Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"TENSOR SPLIT RECOMMENDATION\")\nprint(\"=\" * 70)\n",
    "\nif gpus:\n",
    "    tensor_split = suggest_tensor_split(gpus)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Recommended Tensor Split for Multi-GPU:\\n\")\n",
    "    \n",
    "    if isinstance(tensor_split, list):\n",
    "        total = sum(tensor_split)\n",
    "        for i, split in enumerate(tensor_split):\n",
    "            pct = split * 100\n",
    "            print(f\"  GPU {i}: {pct:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìù Explanation:\")\n",
    "        print(f\"  - Each GPU receives a fraction of model layers\")\n",
    "        print(f\"  - Proportional to VRAM size\")\n",
    "        print(f\"  - Requires NCCL for GPU communication\")\n",
    "        print(f\"  - Use with: LlamaServer(..., tensor_split={tensor_split})\")\n",
    "    else:\n",
    "        print(f\"  Single GPU - No tensor split needed\")\n",
    "        print(f\"\\nüìù Explanation:\")\n",
    "        print(f\"  - Only 1 GPU detected\")\n",
    "        print(f\"  - All model layers fit on one GPU\")\nelse:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: GPU Layer Distribution Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"GPU LAYER DISTRIBUTION CALCULATOR\")\nprint(\"=\" * 70)\n",
    "\nif gpus:\n",
    "    # Example model: 32 layers\n",
    "    total_layers = 32\n",
    "    total_vram = sum(gpu['memory_mb'] for gpu in gpus)\n",
    "    \n",
    "    print(f\"\\nExample: Model with {total_layers} layers\\n\")\n",
    "    \n",
    "    if isinstance(suggest_tensor_split(gpus), list):\n",
    "        tensor_split = suggest_tensor_split(gpus)\n",
    "        print(f\"Layer Distribution Across {len(gpus)} GPUs:\\n\")\n",
    "        \n",
    "        for i, split in enumerate(tensor_split):\n",
    "            layers = int(total_layers * split)\n",
    "            print(f\"  GPU {i}: {layers} layers ({split*100:.1f}% of model)\")\n",
    "    else:\n",
    "        print(f\"Layer Distribution (Single GPU):\\n\")\n",
    "        print(f\"  GPU 0: {total_layers} layers (100% of model)\")\nelse:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: GPU Utilization with nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"REAL-TIME GPU STATUS (nvidia-smi)\")\nprint(\"=\" * 70)\n",
    "\ntry:\n",
    "    print(\"\\n\")\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free,temperature.gpu,utilization.gpu,utilization.memory',\n",
    "         '--format=csv,noheader'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            parts = [p.strip() for p in line.split(',')]\n",
    "            if len(parts) >= 9:\n",
    "                idx, name, driver, total_mem, used_mem, free_mem, temp, gpu_util, mem_util = parts[:9]\n",
    "                print(f\"GPU {idx}: {name}\")\n",
    "                print(f\"  Driver: {driver}\")\n",
    "                print(f\"  Memory: {used_mem} / {total_mem} (Free: {free_mem})\")\n",
    "                print(f\"  Temperature: {temp}\")\n",
    "                print(f\"  GPU Utilization: {gpu_util}\")\n",
    "                print(f\"  Memory Utilization: {mem_util}\")\n",
    "                print()\n",
    "    else:\n",
    "        print(f\"Error running nvidia-smi: {result.stderr}\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"nvidia-smi timed out\")\nexcept FileNotFoundError:\n",
    "    print(\"nvidia-smi not found - install NVIDIA drivers\")\nexcept Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTip: Run 'nvidia-smi' in terminal to check GPU status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Configuration Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"CONFIGURATION RECOMMENDATIONS\")\nprint(\"=\" * 70)\n",
    "\nif gpus:\n",
    "    print(f\"\\nüìã Recommended LlamaServer Configuration:\\n\")\n",
    "    \n",
    "    total_vram = sum(gpu['memory_mb'] for gpu in gpus)\n",
    "    total_vram_gb = total_vram / 1024\n",
    "    \n",
    "    # Determine n_ctx based on VRAM\n",
    "    if total_vram_gb < 4:\n",
    "        n_ctx = 1024\n",
    "        n_gpu_layers = 20\n",
    "    elif total_vram_gb < 8:\n",
    "        n_ctx = 2048\n",
    "        n_gpu_layers = 33\n",
    "    else:\n",
    "        n_ctx = 4096\n",
    "        n_gpu_layers = 40\n",
    "    \n",
    "    tensor_split = suggest_tensor_split(gpus)\n",
    "    \n",
    "    print(f\"server = LlamaServer(\")\n",
    "    print(f\"    model_path=\\\"path/to/model.gguf\\\",\")\n",
    "    print(f\"    n_gpu_layers={n_gpu_layers},  # GPU layers (higher = more GPU)\")\n",
    "    print(f\"    n_ctx={n_ctx},  # Context size (based on {total_vram_gb:.1f} GB VRAM)\")\n",
    "    print(f\"    n_threads=4,  # CPU threads\")\n",
    "    \n",
    "    if isinstance(tensor_split, list) and len(tensor_split) > 1:\n",
    "        print(f\"    tensor_split=[{', '.join(f'{s:.2f}' for s in tensor_split)}],  # Multi-GPU\")\n",
    "    \n",
    "    print(f\")\")\n",
    "    \n",
    "    print(f\"\\nüí° Tips:\")\n",
    "    print(f\"  ‚Ä¢ n_gpu_layers=33 means offload 33 layers to GPU\")\n",
    "    print(f\"  ‚Ä¢ Higher n_gpu_layers = faster but uses more VRAM\")\n",
    "    print(f\"  ‚Ä¢ Adjust n_ctx if you run out of memory\")\n",
    "    print(f\"  ‚Ä¢ For multi-GPU, tensor_split distributes layers\")\nelse:\n",
    "    print(\"No GPUs detected - CPU-only mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"TROUBLESHOOTING GUIDE\")\nprint(\"=\" * 70)\n",
    "\nprint(f\"\\n‚ùå Issue: No GPUs detected\")\nprint(f\"\\nSolutions:\")\nprint(f\"  1. Check GPU installed: lspci | grep NVIDIA\")\nprint(f\"  2. Install NVIDIA drivers: nvidia-driver package\")\nprint(f\"  3. Install CUDA: Download from nvidia.com\")\nprint(f\"  4. Verify: Run 'nvidia-smi'\")\n",
    "\nprint(f\"\\n‚ùå Issue: Out of Memory (OOM) errors\")\nprint(f\"\\nSolutions:\")\nprint(f\"  1. Reduce n_gpu_layers (offload fewer layers)\")\nprint(f\"  2. Reduce n_ctx (smaller context)\")\nprint(f\"  3. Use smaller model (fewer parameters)\")\nprint(f\"  4. Enable tensor split for multi-GPU\")\n",
    "\nprint(f\"\\n‚ùå Issue: Slow inference\")\nprint(f\"\\nSolutions:\")\nprint(f\"  1. Increase n_gpu_layers (offload more to GPU)\")\nprint(f\"  2. Check GPU utilization: nvidia-smi\")\nprint(f\"  3. Use larger batch size if applicable\")\nprint(f\"  4. Add more GPUs for tensor parallelism\")\n",
    "\nprint(f\"\\n‚ùå Issue: NCCL errors (multi-GPU)\")\nprint(f\"\\nSolutions:\")\nprint(f\"  1. Ensure all GPUs are from same vendor (NVIDIA)\")\nprint(f\"  2. Check NCCL is installed: pip show nccl\")\nprint(f\"  3. Verify tensor_split format is correct\")\nprint(f\"  4. Use nvidia-smi to check GPU connectivity\")\n",
    "\nprint(f\"\\nüí¨ Getting Help:\")\nprint(f\"  ‚Ä¢ GitHub Issues: https://github.com/Local-Llama-Inference/...\")\nprint(f\"  ‚Ä¢ Documentation: Check README.md\")\nprint(f\"  ‚Ä¢ Run: llama-inference info  (for debugging)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **GPU Detection**: Use `detect_gpus()` to get GPU info\n",
    "2. **Compute Capability**: Determines feature support (sm_50+)\n",
    "3. **VRAM**: Limits model size and context length\n",
    "4. **Tensor Split**: Distributes models across multiple GPUs\n",
    "5. **Optimization**: Balance GPU layers, context size, and batch size\n",
    "6. **Monitoring**: Use `nvidia-smi` to check utilization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Run `01_quick_start.ipynb` to start generating text\n",
    "- Try different models and configurations\n",
    "- Monitor GPU usage during inference\n",
    "- Optimize parameters for your use case\n",
    "\n",
    "## Useful Commands\n",
    "\n",
    "```bash\n",
    "# Check GPU\n",
    "nvidia-smi\n",
    "\n",
    "# Real-time GPU monitoring\n",
    "nvidia-smi dmon\n",
    "\n",
    "# Check CUDA version\n",
    "nvcc --version\n",
    "\n",
    "# Package info\n",
    "llama-inference info\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
