# Local-Llama-Inference
Local-Llama-Inference is a python sdk with built-in integration of llama.cpp and nccl for end users who want to run GGUF LLM models in Nvidia GPUs (single or multiple).
